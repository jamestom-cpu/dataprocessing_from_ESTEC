{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9135ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_packages import signal_elaboration as s\n",
    "from my_packages.utils import probes, probes_walk, HandlePaths\n",
    "from my_packages.directory_data import  GetCoordinates, make_generator, make_all_generators, RootBatchGen\n",
    "from my_packages.my_hdf5 import explore_library, see_groups_and_datasets\n",
    "from my_packages.signal_elaboration import read_csv\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import scipy.signal\n",
    "import scipy.io\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f29564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have the following probes ['xfb31', 'XFE_04s', 'XFR31']\n",
      "the structure in which the files are saved is: \n",
      "{'XFE_04s': {'r18': {}, 'r18_o': {}},\n",
      " 'XFR31': {'r18_o': {'along_x': {}, 'along_y': {}}},\n",
      " 'xfb31': {'r18': {}}}\n",
      "{'XFE_04s': ['E:\\\\\\\\XFE_04s/r18', 'E:\\\\\\\\XFE_04s/r18_o'],\n",
      " 'XFR31': ['E:\\\\\\\\XFR31/r18_o/along_x', 'E:\\\\\\\\XFR31/r18_o/along_y'],\n",
      " 'xfb31': ['E:\\\\\\\\xfb31/r18']}\n",
      "['E:\\\\\\\\/xfb31/r18',\n",
      " 'E:\\\\\\\\/XFE_04s/r18',\n",
      " 'E:\\\\\\\\/XFE_04s/r18_o',\n",
      " 'E:\\\\\\\\/XFR31/r18_o/along_x',\n",
      " 'E:\\\\\\\\/XFR31/r18_o/along_y']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# exploring the NAS\n",
    "# you can explore the NAS using cmd line tools:\n",
    "\n",
    "print(\"we have the following probes\", probes)\n",
    "print(\"the structure in which the files are saved is: \")\n",
    "pprint(probes_walk)\n",
    "\n",
    "external_drive = r\"E:\\\\\"\n",
    "\n",
    "# obtain all possible paths from the json-like structure\n",
    "data_paths = HandlePaths(base_path=external_drive)(probes_walk)\n",
    "probe_paths = {probe: HandlePaths(base_path=os.path.join(external_drive,probe))(probes_walk[probe]) for probe in probes}\n",
    "all_paths = HandlePaths(base_path=external_drive)(probes_walk) \n",
    "pprint(probe_paths)  \n",
    "pprint(all_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93829e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## create an object for the probes that can handle all the measurements for each probe\n",
    "file = \"measurements.h5\"\n",
    "generators = make_all_generators(file, return_fullpaths=True)\n",
    "              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'XFE_04s': <my_packages.directory_data.RootBatchGen object at 0x000001E305D70190>,\n",
      " 'XFR31_along_x': <my_packages.directory_data.RootBatchGen object at 0x000001E346DAB7F0>,\n",
      " 'XFR31_along_y': <my_packages.directory_data.RootBatchGen object at 0x000001E305D018E0>,\n",
      " 'xfb31': <my_packages.directory_data.RootBatchGen object at 0x000001E305D2D3D0>}\n",
      "{'creation date': '21/06/2023 21:01:18',\n",
      " 'measurement_path': 'E:/XFE_04s/r18_o',\n",
      " 'probe': 'XFE_04s'}\n",
      "['measurement_points', 'x_coordinates', 'y_coordinates']\n",
      "E:/XFE_04s/r18_o\n"
     ]
    }
   ],
   "source": [
    "pprint(generators)\n",
    "\n",
    "#next(generators[\"XFE_04s\"])\n",
    "\n",
    "group = \"XFE_04s\"\n",
    "\n",
    "gen = generators[\"XFE_04s\"]\n",
    "\n",
    "f = h5py.File(file, \"r\")\n",
    "\n",
    "gr = f[group]\n",
    "pprint(dict(gr.attrs))\n",
    "\n",
    "print(list(gr[\"coordinates\"].keys()))\n",
    "\n",
    "x = np.array(gr[\"coordinates\"][\"x_coordinates\"])\n",
    "y = np.array(gr[\"coordinates\"][\"y_coordinates\"])\n",
    "\n",
    "print(gr.attrs['measurement_path'])\n",
    "\n",
    "\n",
    "batch = next(gen)\n",
    "\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "514c9c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x86000.00y20500.001.csv',\n",
       " 'x86000.00y20500.002.csv',\n",
       " 'x86000.00y20500.003.csv',\n",
       " 'x86000.00y20500.004.csv',\n",
       " 'x86000.00y20500.005.csv',\n",
       " 'x86000.00y20500.006.csv',\n",
       " 'x86000.00y20500.007.csv',\n",
       " 'x86000.00y20500.008.csv',\n",
       " 'x86000.00y20500.009.csv',\n",
       " 'x86000.00y20500.0010.csv',\n",
       " 'x86000.00y20500.0011.csv',\n",
       " 'x86000.00y20500.0012.csv',\n",
       " 'x86000.00y20500.0013.csv',\n",
       " 'x86000.00y20500.0014.csv',\n",
       " 'x86000.00y20500.0015.csv',\n",
       " 'x86000.00y20500.0016.csv',\n",
       " 'x86000.00y20500.0017.csv',\n",
       " 'x86000.00y20500.0018.csv',\n",
       " 'x86000.00y20500.0019.csv',\n",
       " 'x86000.00y20500.0020.csv',\n",
       " 'x86000.00y20500.0021.csv',\n",
       " 'x86000.00y20500.0022.csv',\n",
       " 'x86000.00y20500.0023.csv',\n",
       " 'x86000.00y20500.0024.csv',\n",
       " 'x86000.00y20500.0025.csv',\n",
       " 'x86000.00y20500.0026.csv',\n",
       " 'x86000.00y20500.0027.csv',\n",
       " 'x86000.00y20500.0028.csv',\n",
       " 'x86000.00y20500.0029.csv',\n",
       " 'x86000.00y20500.0030.csv',\n",
       " 'x86000.00y20500.0031.csv',\n",
       " 'x86000.00y20500.0032.csv',\n",
       " 'x86000.00y20500.0033.csv',\n",
       " 'x86000.00y20500.0034.csv',\n",
       " 'x86000.00y20500.0035.csv',\n",
       " 'x86000.00y20500.0036.csv',\n",
       " 'x86000.00y20500.0037.csv',\n",
       " 'x86000.00y20500.0038.csv',\n",
       " 'x86000.00y20500.0039.csv',\n",
       " 'x86000.00y20500.0040.csv',\n",
       " 'x86000.00y20500.0041.csv',\n",
       " 'x86000.00y20500.0042.csv',\n",
       " 'x86000.00y20500.0043.csv',\n",
       " 'x86000.00y20500.0044.csv',\n",
       " 'x86000.00y20500.0045.csv',\n",
       " 'x86000.00y20500.0046.csv',\n",
       " 'x86000.00y20500.0047.csv',\n",
       " 'x86000.00y20500.0048.csv',\n",
       " 'x86000.00y20500.0049.csv',\n",
       " 'x86000.00y20500.0050.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.current_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59eedf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "f1 = batch[0]\n",
    "r = np.array(list(read_csv(f1)))\n",
    "\n",
    "print(r.shape)\n",
    "\n",
    "rr = np.rec.fromarrays(list(r.transpose()), dtype=[(\"time\", \"f\"), (\"amplitude\", \"f\")])\n",
    "\n",
    "df = pd.DataFrame(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "380c6984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from my_packages.my_hdf5 import *\n",
    "\n",
    "fp = \"test_batch.h5\"\n",
    "\n",
    "def measurements2dataset(library, batch_generator, dataset_name, compression_level=4):\n",
    "    number_of_batches = len(batch_generator)\n",
    "    datapoints = int(100e3)\n",
    "    batch_size = batch_generator.batch_size\n",
    "    # batch_size = batch_generator.batch_size\n",
    "\n",
    "    dataset_shape = (batch_size, number_of_batches, 2, datapoints)\n",
    "\n",
    "    dataset_attr = dict(\n",
    "        observation_interval = \"10us\",\n",
    "        number_of_points = datapoints,\n",
    "        max_frequency_resolution = 1/datapoints, \n",
    "        structure_of_numpy_array = (\"batch_size\", \"number_of_batches\", \"time, amplitude\", \"datapoints\") \n",
    "    )\n",
    "\n",
    "    if not exists(library):\n",
    "        build_hdf5(name=library)\n",
    "\n",
    "    with h5py.File(library, \"a\") as f:\n",
    "        dataset = f.require_dataset(\n",
    "            dataset_name, \n",
    "            shape=dataset_shape, \n",
    "            dtype=\"float64\", \n",
    "            chunks=(batch_size, 1, 2, datapoints), \n",
    "            compression=compression_level\n",
    "            )\n",
    "        for batch in tqdm(batch_generator):\n",
    "            dataset[:, batch_generator.number_of_batches_done, :, : ] = get_numpy_from_batch(batch)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def get_numpy_from_batch(batch):\n",
    "    readings = [list(read_csv(csv_file_path)) for csv_file_path in batch]\n",
    "\n",
    "    # I must transpose so that the shape has the number of points in the last position and 2 - ie time and amplitude - in \n",
    "    # second to last place\n",
    "    \n",
    "    return np.array(readings).transpose(0,2,1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69028087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 18/2166 [04:24<8:45:08, 14.67s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m measurements2dataset(\u001b[39m\"\u001b[39;49m\u001b[39mtest_batch\u001b[39;49m\u001b[39m\"\u001b[39;49m, gen, \u001b[39m\"\u001b[39;49m\u001b[39mtest1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[9], line 34\u001b[0m, in \u001b[0;36mmeasurements2dataset\u001b[1;34m(library, batch_generator, dataset_name, compression_level)\u001b[0m\n\u001b[0;32m     26\u001b[0m dataset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mrequire_dataset(\n\u001b[0;32m     27\u001b[0m     dataset_name, \n\u001b[0;32m     28\u001b[0m     shape\u001b[39m=\u001b[39mdataset_shape, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     compression\u001b[39m=\u001b[39mcompression_level\n\u001b[0;32m     32\u001b[0m     )\n\u001b[0;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(batch_generator):\n\u001b[1;32m---> 34\u001b[0m     dataset[:, batch_generator\u001b[39m.\u001b[39mnumber_of_batches_done, :, : ] \u001b[39m=\u001b[39m get_numpy_from_batch(batch)\n",
      "Cell \u001b[1;32mIn[9], line 42\u001b[0m, in \u001b[0;36mget_numpy_from_batch\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_numpy_from_batch\u001b[39m(batch):\n\u001b[1;32m---> 42\u001b[0m     readings \u001b[39m=\u001b[39m [\u001b[39mlist\u001b[39m(read_csv(csv_file_path)) \u001b[39mfor\u001b[39;00m csv_file_path \u001b[39min\u001b[39;00m batch]\n\u001b[0;32m     44\u001b[0m     \u001b[39m# I must transpose so that the shape has the number of points in the last position and 2 - ie time and amplitude - in \u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \u001b[39m# second to last place\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(readings)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_numpy_from_batch\u001b[39m(batch):\n\u001b[1;32m---> 42\u001b[0m     readings \u001b[39m=\u001b[39m [\u001b[39mlist\u001b[39;49m(read_csv(csv_file_path)) \u001b[39mfor\u001b[39;00m csv_file_path \u001b[39min\u001b[39;00m batch]\n\u001b[0;32m     44\u001b[0m     \u001b[39m# I must transpose so that the shape has the number of points in the last position and 2 - ie time and amplitude - in \u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \u001b[39m# second to last place\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(readings)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tomas\\Desktop\\phd\\dataprocessing\\my_packages\\signal_elaboration.py:174\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_csv\u001b[39m(filename):\n\u001b[1;32m--> 174\u001b[0m     \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m \u001b[39mopen\u001b[39m(filename):\n\u001b[0;32m    175\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: \u001b[39mfloat\u001b[39m(x),row\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]))\n",
      "File \u001b[1;32mc:\\Users\\tomas\\miniconda3\\envs\\data_processing\\lib\\codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_buffer_decode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, errors, final):\n\u001b[0;32m    315\u001b[0m     \u001b[39m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[39m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m    322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer_decode(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors, final)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "measurements2dataset(\"test_batch\", gen, \"test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dfda99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"test_batch\", \"r\") as f:\n",
    "    arr = f[\"test1\"][:, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7d95ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2, 100000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8f59d9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m measurements2dataset(fp, batch, \u001b[39m\"\u001b[39;49m\u001b[39mbatch\u001b[39;49m\u001b[39m\"\u001b[39;49m, compression_level\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m \u001b[39mwith\u001b[39;00m h5py\u001b[39m.\u001b[39mFile(fp, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(f[\u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m4\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m20000\u001b[39m])\n",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m, in \u001b[0;36mmeasurements2dataset\u001b[1;34m(library, batch_generator, dataset_name, compression_level)\u001b[0m\n\u001b[0;32m      8\u001b[0m number_of_batches \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch_generator)\n\u001b[0;32m      9\u001b[0m datapoints \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m100e3\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m batch_size \u001b[39m=\u001b[39m batch_generator\u001b[39m.\u001b[39;49mbatch_size\n\u001b[0;32m     11\u001b[0m \u001b[39m# batch_size = batch_generator.batch_size\u001b[39;00m\n\u001b[0;32m     13\u001b[0m dataset_shape \u001b[39m=\u001b[39m (batch_size, number_of_batches, \u001b[39m2\u001b[39m, datapoints)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'batch_size'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f6120cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(fp, \"a\") as f:\n",
    "    f[\"batch\"][:,:,:50000] = np.ones((50,2,50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6c3b675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xfb31.h5', 'measurements.h5']\n",
      "{'dataset_keys': ['measurement_points'],\n",
      " 'group_keys': ['XFE_04s', 'XFR31', 'xfb31']}\n",
      "{'dataset_keys': [], 'group_keys': []}\n"
     ]
    }
   ],
   "source": [
    "from my_packages.my_hdf5 import *\n",
    "from pprint import pprint\n",
    "\n",
    "print(get_all_h5())\n",
    "pprint(see_groups_and_datasets(\"measurements.h5\"))\n",
    "pprint(see_groups_and_datasets(\"measurements.h5\", \"XFE_04s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4a70fe55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coordinates']\n"
     ]
    }
   ],
   "source": [
    "def save_measurement_info(library, dh, probes, measurement_info={}, group_info={}):\n",
    "    path = dh.path\n",
    "    probe = get_probe_from_path(probes, dh.path)\n",
    "    xcoord = dh.coordinates[\"x\"]; ycoord = dh.coordinates[\"y\"] \n",
    "    if not exists(library):\n",
    "        build_hdf5(name=library, groups=[probe])\n",
    "    \n",
    "    if not group_exist(library, probe):\n",
    "        add_group(library, probe, **group_info)\n",
    "\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "    # open the group\n",
    "    with h5py.File(library, \"a\") as f:\n",
    "        g = f[probe]\n",
    "        # create the dataset\n",
    "        # require_dataset is the same as create_dataset. However, if the dataset already exists it does not overwirte.\n",
    "\n",
    "        #check if the coordinate group already exists\n",
    "        group_keys = [key for key, items in g.items() if isinstance(items, h5py.Group)]\n",
    "        print(group_keys)\n",
    "        if \"coordinates\" in group_keys:\n",
    "            res = input(\"type y to overwrite\")\n",
    "\n",
    "            if res != \"y\":\n",
    "                return \n",
    "            else:\n",
    "                del g[\"coordinates\"]\n",
    "\n",
    "       \n",
    "\n",
    "        coord_gr = g.create_group(\"coordinates\")\n",
    "        coord_gr.attrs[\"creation date\"]= dt_string\n",
    "        coord_gr.attrs[\"measurement_path\"] = path\n",
    "        coord_gr.attrs[\"description\"] = \\\n",
    "        \"These coordinates were obtained as the coordinates that appear atleast once among the \\\n",
    "        measurement points as found in the names of the csv files\"\n",
    "\n",
    "        x_ds=coord_gr.require_dataset(\"x_coordinates\", shape=xcoord.shape, dtype=np.float32, data=xcoord)\n",
    "        y_ds=coord_gr.require_dataset(\"y_coordinates\", shape=ycoord.shape, dtype=np.float32, data=ycoord)\n",
    "        points = coord_gr.require_dataset(\"measurement_points\", shape=dh.points.shape, dtype=np.float32, data=dh.points)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def get_probe_from_path(probes, path):\n",
    "    probe_ = [p for p in probes if p in path]\n",
    "    # check there is one element in the probe list\n",
    "    try:\n",
    "        probe = (lambda x: x)(*probe_)\n",
    "    except:\n",
    "        raise(\"probe length is \", len(probe_))\n",
    "    return probe\n",
    "\n",
    "save_measurement_info(\"measurements.h5\", dh, probes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "051bd4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_keys': ['measurement_points'],\n",
      " 'group_keys': ['XFE_04s', 'XFR31', 'test', 'xfb31']}\n",
      "True\n",
      "['XFE_04s', 'XFR31', 'test', 'xfb31']\n"
     ]
    }
   ],
   "source": [
    "path= \"measurements.h5\"\n",
    "pprint(see_groups_and_datasets(path))\n",
    "print(group_exist(path, \"test\"))\n",
    "\n",
    "remove_group(path, \"test\")\n",
    "add_group(path, \"test\", description = \"this group is a test\", owner=\"tomas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d20dbbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"measurements.h5\"\n",
    "\n",
    "\n",
    "def explore_library(path, recursive=True):\n",
    "    def printall(name, obj):\n",
    "        print(\"NAME: {:^30}\".format(name))\n",
    "        print(\"Type: {:^20}\".format(f\"GROUP - Subgroups: {list(obj.keys())}\" if isinstance(obj, h5py.Group) else \"DATASET\"))\n",
    "        print(\"Parent Path: {:<10}\".format(obj.parent.name))\n",
    "        print(\"Attributes: \")\n",
    "        pprint(dict(obj.attrs))\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            print(\"shape: \", obj.shape, \"____ dtype: \", obj.dtype) \n",
    "        print(\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        if recursive:\n",
    "            f.visititems(printall)\n",
    "        else:\n",
    "            for name, obj in f.items():\n",
    "                printall(name, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bd3701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:             xfb31             \n",
      "Type: GROUP - Subgroups: ['coordinates']\n",
      "Parent Path: /         \n",
      "Attributes: \n",
      "{}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NAME:       xfb31/coordinates       \n",
      "Type: GROUP - Subgroups: ['measurement_points', 'x_coordinates', 'y_coordinates']\n",
      "Parent Path: /xfb31    \n",
      "Attributes: \n",
      "{'creation date': '14/09/2022 21:26:36',\n",
      " 'description': 'These coordinates were obtained as the coordinates that '\n",
      "                'appear atleast once among the         measurement points as '\n",
      "                'found in the names of the csv files',\n",
      " 'measurement_path': '/NAS/xfb31/r18'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NAME: xfb31/coordinates/measurement_points\n",
      "Type:       DATASET       \n",
      "Parent Path: /xfb31/coordinates\n",
      "Attributes: \n",
      "{}\n",
      "shape:  (2166,) ____ dtype:  float32\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NAME: xfb31/coordinates/x_coordinates\n",
      "Type:       DATASET       \n",
      "Parent Path: /xfb31/coordinates\n",
      "Attributes: \n",
      "{}\n",
      "shape:  (57,) ____ dtype:  float32\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NAME: xfb31/coordinates/y_coordinates\n",
      "Type:       DATASET       \n",
      "Parent Path: /xfb31/coordinates\n",
      "Attributes: \n",
      "{}\n",
      "shape:  (38,) ____ dtype:  float32\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explore_library(\"measurements.h5\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cee70a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date and time = 14/09/2022 17:13:29\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(\"date and time =\", dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bdf3cb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2166, 2)\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File(\"measurements.h5\", \"r\")\n",
    "\n",
    "for k, obj in f.items():\n",
    "    if isinstance(obj, h5py.Dataset):\n",
    "        print(obj.shape)\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "240f6e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dh.coordinates[\"x\"]\n",
    "y = dh.coordinates[\"y\"]\n",
    "\n",
    "cc = np.rec.fromarrays([dh.points[:,0]/1e3, dh.points[:,1]/1e3], dtype=[(\"x\", \"float16\"), (\"y\", \"float16\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e3c73ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134.0</td>\n",
       "      <td>34.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.0</td>\n",
       "      <td>64.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124.0</td>\n",
       "      <td>50.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148.0</td>\n",
       "      <td>64.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172.0</td>\n",
       "      <td>44.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>182.0</td>\n",
       "      <td>34.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2162</th>\n",
       "      <td>122.0</td>\n",
       "      <td>70.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>182.0</td>\n",
       "      <td>32.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164</th>\n",
       "      <td>178.0</td>\n",
       "      <td>46.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165</th>\n",
       "      <td>158.0</td>\n",
       "      <td>94.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2166 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          x     y\n",
       "0     134.0  34.5\n",
       "1      88.0  64.5\n",
       "2     124.0  50.5\n",
       "3     148.0  64.5\n",
       "4     172.0  44.5\n",
       "...     ...   ...\n",
       "2161  182.0  34.5\n",
       "2162  122.0  70.5\n",
       "2163  182.0  32.5\n",
       "2164  178.0  46.5\n",
       "2165  158.0  94.5\n",
       "\n",
       "[2166 rows x 2 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d72285e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /opt/bitnami/miniconda/lib/python3.7/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/bitnami/miniconda/lib/python3.7/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/bitnami/miniconda/lib/python3.7/site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/bitnami/miniconda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.3.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d523bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
